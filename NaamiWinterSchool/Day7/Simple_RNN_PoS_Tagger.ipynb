{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RNN PoS Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "import time\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "## Setting up a seed value ensures that each time you run the model you get same random initializations. \n",
    "## i.e. same results each time you run the model. \n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "## The dataset is structured as it is divided into 3 sections, and to store these three sections\n",
    "## into three different objects, we make 3 fields: TEXT, UD_TAGS and PTB_TAGS on which we will be loading our dataset. \n",
    "TEXT = data.Field(lower = True)\n",
    "UD_TAGS = data.Field(unk_token = None)\n",
    "PTB_TAGS = data.Field(unk_token = None)\n",
    "fields = ((\"text\", TEXT), (\"udtags\", UD_TAGS), (\"ptbtags\", PTB_TAGS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading en-ud-v2.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "en-ud-v2.zip: 100%|██████████| 688k/688k [00:01<00:00, 421kB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting\n",
      "Number of training examples: 12543\n",
      "Number of validation examples: 2002\n",
      "Number of testing examples: 2077\n"
     ]
    }
   ],
   "source": [
    "## Loading the UDPOS dataset, and dividing it into training, validation and testing datasets. \n",
    "train_data, valid_data, test_data = datasets.UDPOS.splits(fields)\n",
    "\n",
    "## You can check length datasets by following command. \n",
    "print(f\"Number of training examples: {len(train_data)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data)}\")\n",
    "print(f\"Number of testing examples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized_text                     : ['two', 'of', 'them', 'were', 'being', 'run', 'by', '2', 'officials', 'of', 'the', 'ministry', 'of', 'the', 'interior', '!']\n",
      "Respective UD POS Tags             : ['NUM', 'ADP', 'PRON', 'AUX', 'AUX', 'VERB', 'ADP', 'NUM', 'NOUN', 'ADP', 'DET', 'PROPN', 'ADP', 'DET', 'PROPN', 'PUNCT']\n",
      "Respective Penn Tree Bank POS Tags : ['CD', 'IN', 'PRP', 'VBD', 'VBG', 'VBN', 'IN', 'CD', 'NNS', 'IN', 'DT', 'NNP', 'IN', 'DT', 'NNP', '.']\n"
     ]
    }
   ],
   "source": [
    "## You can look at the dataset which we have loaded by using the below commands. \n",
    "print('Tokenized_text                     :',(train_data.examples[3].text))\n",
    "print('Respective UD POS Tags             :',(train_data.examples[3].udtags))\n",
    "print('Respective Penn Tree Bank POS Tags :',(train_data.examples[3]).ptbtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [12:27, 1.15MB/s]                               \n",
      "100%|█████████▉| 399907/400000 [00:40<00:00, 6675.30it/s] "
     ]
    }
   ],
   "source": [
    "'''\n",
    "########################################################################################################################################################\n",
    "## This cell will take around 10 minutes to run, as it downloads the glove.6B.100d vocabulary locally before using it. And it's size is around 822MB. ##\n",
    "########################################################################################################################################################\n",
    "'''\n",
    "\n",
    "## The data currently is in string format, and as we know neural network only take numbers as input. \n",
    "\n",
    "## So, we need to map the words to certain numerical space where it's semantic remains intact and the\n",
    "## corresponding vectors in that space will be representation of the corresponding words and are called \"WORD VECTORS\" or \"WORD EMBEDDINGS\".\n",
    "\n",
    "## Their are several techniques available to find good quality word vectors. But we would be just taking the word vectors from a standardized source.\n",
    "## GLOVE is a standardized vocabulary of word vectors. \n",
    "\n",
    "## MIN_FREQ denotes that the minimum frequency of words in your dataset you would like to consider, or upload word embeddings for. \n",
    "MIN_FREQ = 2\n",
    "\n",
    "## The command below extract the vocabulary of word embeddings of all the words occuring in the training data whose frequency is more then MIN_FREQ.\n",
    "TEXT.build_vocab(train_data, \n",
    "                 min_freq = MIN_FREQ,\n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "\n",
    "## The below command forms the vocabulary of all the tags availble in the training dataset. \n",
    "UD_TAGS.build_vocab(train_data)\n",
    "PTB_TAGS.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can check the number of unique words in your vocabulary.\n",
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in UD_TAG vocabulary: {len(UD_TAGS.vocab)}\")\n",
    "print(f\"Unique tokens in PTB_TAG vocabulary: {len(PTB_TAGS.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can look at the 20 most frequent words occuring in your dataset, and the set of all POS tags available in the dataset.\n",
    "print('20 most frequent words in vocabulary :' ,TEXT.vocab.freqs.most_common(20))\n",
    "print('All the POS tags available           :',UD_TAGS.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A utlity function to calculate the percentage of all the tag_counts. \n",
    "def tag_percentage(tag_counts):    \n",
    "    total_count = sum([count for tag, count in tag_counts])\n",
    "    tag_counts_percentages = [(tag, count, count/total_count) for tag, count in tag_counts]\n",
    "    return tag_counts_percentages\n",
    "\n",
    "print(\"Tag  Occurences Percentage\\n\")\n",
    "for tag, count, percent in tag_percentage(UD_TAGS.vocab.freqs.most_common()):\n",
    "    print(f\"{tag}\\t{count}\\t{percent*100:4.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNPOSTagger(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size, \n",
    "                 embedding_dim, \n",
    "                 hidden_dim, \n",
    "                 output_dim, \n",
    "                 n_layers, \n",
    "                 bidirectional, \n",
    "                 dropout, \n",
    "                 pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.rnn = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers = n_layers, \n",
    "                           bidirectional = bidirectional)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "\n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        #output = [sent len, batch size, hid dim * n directions]\n",
    "        #hidden/cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        predictions = self.fc(self.dropout(outputs))\n",
    "        \n",
    "        #predictions = [sent len, batch size, output dim]\n",
    "        \n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NETWORK ARCHITECTURE DEFINITION\n",
    "\n",
    "# LOOK UP THE DOCUMENTATION OF EACH TO FILL IN THE PARAMETERS\n",
    "class RNNPOSTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "\n",
    "        '''\n",
    "        Here, \n",
    "        vocab_size    : Size of the input vocabulary you are using. \n",
    "        embedding_dim : Size of the word embeddings being used. \n",
    "        hidden_dim    : Hidden Layer dimensions. \n",
    "        output_dim    : Dimension of the output you require. \n",
    "        n_layers      : Number of stacked up recurrent layers you require \n",
    "        bidirectional : Whether you want your network to be bidirectional or not. \n",
    "        dropout       : Dropout rate you want.\n",
    "        pad_idx       : Padding index, padding occurs if it is 1 else not. \n",
    "        '''\n",
    "\n",
    "        ### TASK 1: Define a generalized network for POS Tagging using above parameters. \n",
    "\n",
    "        ## Define embedding layer: It takes your one hot vectors to your less dimensional word embeddings. \n",
    "        self.embedding = nn.Embedding() \n",
    "\n",
    "        ## Define LSTM layer. \n",
    "        self.rnn = nn.LSTM()\n",
    "\n",
    "        ## Final fully connected layer to map hidden space to output space.  \n",
    "        ## Note that you have to consider whether or not your network is bidirectional for defining the layer dimensions. \n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "\n",
    "        ## This layer define the drop out rate. (No need to do anything)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, text):\n",
    "        \n",
    "        ## TASK 2: Define the forward pass of the network. \n",
    "\n",
    "        ## text_dimensions = [sent len, batch size] \n",
    "        ## Add the initial input inside the self.dropout, it will allow dropouts in this layer.\n",
    "        embedded = self.dropout()\n",
    "        #embedded_dimensions = [sent len, batch size, emb dim]\n",
    "        \n",
    "        ## Forward pass through LSTM Layer. \n",
    "        outputs, (hidden, cell) =  self.rnn(embedded)\n",
    "        ## output_dimensions = [sent len, batch size, hid dim * n directions]\n",
    "        ## hidden/cell_dimensions = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        # Final Output Layer\n",
    "        predictions =  self.fc(self.dropout(outputs))\n",
    "        #predictions = [sent len, batch size, output dim]\n",
    "        \n",
    "        return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUT_DIM is the length of one-hot vectors of size of your Vocabulary. \n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "\n",
    "## Embedding dimensions is the dimension of pretrained embeddings we have.\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "## The Hidden Layer Dimensions. \n",
    "HIDDEN_DIM = 128\n",
    "\n",
    "## THe dimension of output we require. \n",
    "OUTPUT_DIM = len(UD_TAGS.vocab)\n",
    "\n",
    "## Number of stacked recurrent layers you require. \n",
    "N_LAYERS = 2\n",
    "\n",
    "## Bidirectional LSTM ensures that we capture both the forward and backward flow of semantics. \n",
    "BIDIRECTIONAL = True\n",
    "\n",
    "## Dropout rate determines proportion of random connections the model can drop for a particular interation.\n",
    "DROPOUT = 0.25\n",
    "\n",
    "## The map from vocab to index. \n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "#print(type(PAD_IDX))\n",
    "\n",
    "model = RNNPOSTagger(INPUT_DIM, \n",
    "                     EMBEDDING_DIM, \n",
    "                     HIDDEN_DIM, \n",
    "                     OUTPUT_DIM, \n",
    "                     N_LAYERS, \n",
    "                     BIDIRECTIONAL, \n",
    "                     DROPOUT, \n",
    "                     PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Weight Initialization Function\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.normal_(param.data, mean=0, std=0.1)\n",
    "model.apply(init_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameter Counting Function\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "print(pretrained_embeddings.shape)\n",
    "\n",
    "## Copying the pretrained embeddings into our model. \n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "## Padding the short sentences to larger size sentences. \n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "print(model.embedding.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optimization technique to be used. \n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "TAG_PAD_IDX = UD_TAGS.vocab.stoi[UD_TAGS.pad_token]\n",
    "## Criterion definition\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)\n",
    "\n",
    "## Transfering the models to the device on which we will train the network\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to calculate the categorical_accuracy. \n",
    "def categorical_accuracy(preds, y, tag_pad_idx):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    ## Here we are taking the Tag with highest confidence as the POS tag of that particular word. \n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
    "    non_pad_elements = (y != tag_pad_idx).nonzero()\n",
    "    correct = max_preds[non_pad_elements].squeeze(1).eq(y[non_pad_elements])\n",
    "    return correct.sum() / torch.FloatTensor([y[non_pad_elements].shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, tag_pad_idx):\n",
    "    \n",
    "    ### TASK 3: Write the training function\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        text = batch.text\n",
    "        tags = batch.udtags\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ''' Run the model on the text input. '''\n",
    "        predictions = \n",
    "      \n",
    "        #predictions = [sent len, batch size, output dim]\n",
    "        #tags = [sent len, batch size]\n",
    "        predictions = predictions.view(-1, predictions.shape[-1])\n",
    "        tags = tags.view(-1)\n",
    "        #predictions = [sent len * batch size, output dim]\n",
    "        #tags = [sent len * batch size]\n",
    "        \n",
    "        ''' Loss Calculation'''\n",
    "        loss = criterion()\n",
    "                \n",
    "        acc = categorical_accuracy(predictions, tags, tag_pad_idx)\n",
    "        \n",
    "        ''' Write command for backpropagation'''\n",
    "        # loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for evaluating our model while training.\n",
    "def evaluate(model, iterator, criterion, tag_pad_idx):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            text = batch.text\n",
    "            tags = batch.udtags\n",
    "            \n",
    "            predictions = model(text)\n",
    "            \n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            tags = tags.view(-1)\n",
    "            \n",
    "            loss = criterion(predictions, tags)\n",
    "            \n",
    "            acc = categorical_accuracy(predictions, tags, tag_pad_idx)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A utility function to calculate the epoch time. \n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Final training loop '''\n",
    "\n",
    "N_EPOCHS = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, TAG_PAD_IDX)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, TAG_PAD_IDX)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Evaluating on test set '''\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion, TAG_PAD_IDX)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
